{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import geopandas as gpd\n",
    "import gpflow\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from glob import glob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 ILLINOIS ILLINOIS\n",
      "1 IOWA IOWA\n",
      "2 MICHIGAN MICHIGAN\n",
      "3 MINNESOTA MINNESOTA\n",
      "4 OHIO OHIO\n",
      "5 INDIANA INDIANA\n",
      "6 WISCONSON WISCONSON\n"
     ]
    }
   ],
   "source": [
    "states = [\"ILLINOIS\", \"IOWA\", \"MICHIGAN\", \"MINNESOTA\", \"OHIO\", \"INDIANA\", \"WISCONSON\"]\n",
    "\n",
    "for i, (X_all, df_features) in enumerate(zip(states, states)):\n",
    "    print(i, X_all, df_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_response = pd.read_csv(\"../../data/synthetic/synthetic_yields.csv\")\n",
    "counties_states = pd.read_csv(\"../../data/crops/counties-states.csv\")\n",
    "df_response = df_response[df_response[\"key\"].isin(counties_states[\"key\"])]\n",
    "# drop duplicated yields\n",
    "df_modis = pd.read_csv(\"../../data/crops/processed_covariates/MOD13Q1-1000-all_data.csv\")\n",
    "df_modis = df_modis[df_modis[\"year\"]==2015]\n",
    "df_modis[\"key\"] = df_modis[\"County\"] + \"-\" + df_modis[\"State\"]\n",
    "df_gridmet = pd.read_csv(\"../../data/crops/processed_covariates/GRIDMET-all_data.csv\")\n",
    "df_gridmet = df_gridmet[df_gridmet[\"year\"]==2015]\n",
    "df_gridmet[\"key\"] = df_gridmet[\"County\"] + \"-\" + df_gridmet[\"State\"]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "dates = ['04-07']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "modis_features = [\"key\", \"longitude\", \"latitude\"] + [f\"EVI_{date}\" for date in dates]\n",
    "df_modis = df_modis[modis_features]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_modis = df_modis.groupby([\"key\"]).mean().reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "col_latlon = [\"longitude\", \"latitude\"]\n",
    "col_modis = [f\"EVI_{date}\" for date in dates]\n",
    "col_gridmet = [f\"pr_{date}\" for date in dates] + [f\"tmmx_{date}\" for date in dates]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "gridmet_features = [\"key\"] + col_gridmet\n",
    "df_gridmet = df_gridmet[gridmet_features]\n",
    "df_gridmet = df_gridmet.groupby([\"key\"]).mean().reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_features = df_modis.merge(df_gridmet, on=[\"key\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_all = df_response.merge(df_features, on=[\"key\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_all.to_csv(\"../../data/crops/centroid_yield_features_data.csv\")\n",
    "df_all.to_csv(\"../../data/synthetic/centroid_synthetic_yield_features_data.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "keys = list(set(df_response.key))\n",
    "keys.sort()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_all = df_all[df_all[\"key\"].isin(keys)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "col_index_space = [0, 1]\n",
    "col_index_modis = [2]\n",
    "col_index_pr = [3]\n",
    "col_index_tmmx = [4]\n",
    "\n",
    "latlon_cols = [\"longitude\", \"latitude\"]\n",
    "modis_cols = [f\"EVI_{date}\" for date in dates]\n",
    "pr_cols = [f\"pr_{date}\" for date in dates] \n",
    "tmmx_cols = [f\"tmmx_{date}\" for date in dates] \n",
    "all_features = latlon_cols + modis_cols + pr_cols + tmmx_cols"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experimental Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"-1\"\n",
    "import gpflow \n",
    "from scipy.stats import sem\n",
    "import json\n",
    "import random \n",
    "import time \n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "seed = 0\n",
    "\n",
    "X = df_all.loc[:,all_features].values\n",
    "# X = df_all.iloc[:,6:34].values\n",
    "y = df_all.y.values[:, None]\n",
    "kf = KFold(n_splits=5, random_state=seed, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Centroid GP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]2022-01-12 12:42:24.101867: E tensorflow/stream_executor/cuda/cuda_driver.cc:271] failed call to cuInit: CUDA_ERROR_NO_DEVICE: no CUDA-capable device is detected\n",
      "2022-01-12 12:42:24.101905: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:169] retrieving CUDA diagnostic information for host: nvidia4\n",
      "2022-01-12 12:42:24.101927: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:176] hostname: nvidia4\n",
      "2022-01-12 12:42:24.102020: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:200] libcuda reported version is: 460.27.4\n",
      "2022-01-12 12:42:24.102039: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:204] kernel reported version is: 460.27.4\n",
      "2022-01-12 12:42:24.102043: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:310] kernel version seems to match DSO: 460.27.4\n",
      "2022-01-12 12:42:24.102355: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-01-12 12:42:24.156297: W tensorflow/python/util/util.cc:368] Sets are not currently considered sequences, but this may change in the future, so consider avoiding using them.\n",
      "5it [00:14,  2.83s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MAPE [0.2294375558558635, 0.3014968315502764, 0.2998569175156853, 0.20309749697917764, 0.310900231788104]\n",
      "RMSE [0.7289501465827158, 0.9270620074110206, 1.094964059114099, 1.086240665381462, 0.8917501630672453]\n",
      "LL [0.8635893470586241, 0.643365766644254, 0.4183723078453546, 0.31524158955644926, 0.6488686199334244]\n",
      "Training Time []\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "RMSE = []\n",
    "MAPE  = []\n",
    "LL  = []\n",
    "training_time = []\n",
    "\n",
    "for fold, (train_index, test_index) in tqdm(enumerate(kf.split(keys))):\n",
    "    # randomly pick counties\n",
    "    train_keys = [keys[key] for key in train_index]\n",
    "    test_keys = [keys[key] for key in test_index]\n",
    "    train_index = df_all[\"key\"].isin(train_keys)\n",
    "    test_index = df_all[\"key\"].isin(test_keys)\n",
    "    X_train, X_test = X[train_index], X[test_index]\n",
    "    y_train, y_test = y[train_index], y[test_index]\n",
    "    scaler_x = StandardScaler().fit(X_train)\n",
    "    scaler_y = StandardScaler().fit(y_train)\n",
    "    X_train, y_train = scaler_x.transform(X_train), scaler_y.transform(y_train)\n",
    "    X_test = scaler_x.transform(X_test)\n",
    "\n",
    "    # fit and train GP regression model\n",
    "    k_space = gpflow.kernels.Matern32(active_dims=col_index_space)\n",
    "    k_modis = gpflow.kernels.RBF(active_dims=col_index_modis)\n",
    "    k_pr = gpflow.kernels.RBF(active_dims=col_index_pr)\n",
    "    k_tmmx = gpflow.kernels.RBF(active_dims=col_index_tmmx)\n",
    "    k = k_space + k_modis + k_pr + k_tmmx\n",
    "    m = gpflow.models.GPR(data=(X_train, y_train), kernel=k, mean_function=None)\n",
    "    t0 = time.time()\n",
    "    opt = gpflow.optimizers.Scipy()\n",
    "    opt_logs = opt.minimize(m.training_loss, m.trainable_variables, options=dict(maxiter=500))\n",
    "    t1 = time.time()\n",
    "\n",
    "    pred, var = m.predict_y(X_test)\n",
    "    pred = scaler_y.inverse_transform(pred)\n",
    "    # pred already inversely transformed, therefore only need to multiple the correction by scale_\n",
    "    lower = np.reshape((1.96 * np.sqrt(var[:,0]))*scaler_y.scale_, y_test.shape)\n",
    "    upper = np.reshape((1.96 * np.sqrt(var[:,0]))*scaler_y.scale_, y_test.shape)\n",
    "    errors = np.concatenate((lower, upper), axis=1)\n",
    "    errors = errors.T\n",
    "    \n",
    "    # compute metrics \n",
    "    loglik = np.mean(m.predict_log_density((X_test, scaler_y.transform(y_test))))\n",
    "    RMSE.append(np.sqrt(np.mean((pred - y_test)**2)))\n",
    "    MAPE.append(np.mean(np.abs(( pred - y_test) / y_test)))\n",
    "    LL.append(loglik)\n",
    "\n",
    "    # plot predictions\n",
    "    plt.figure(figsize=(8,8))\n",
    "    plt.scatter(pred, y_test, color=\"red\")\n",
    "    plt.plot(np.linspace(-100,100,201), np.linspace(-100,100,201), color=\"black\")\n",
    "\n",
    "    plt.errorbar(#\n",
    "        pred[:,0],\n",
    "        y_test[:,0],\n",
    "        xerr=errors,\n",
    "        fmt=\"o\",\n",
    "        ls=\"none\",\n",
    "        capsize=5,\n",
    "        markersize=4,\n",
    "        color=\"blue\",\n",
    "        alpha=0.2\n",
    "        )\n",
    "    plt.xlim((y_test.min()-0.5, y_test.max()+0.5))\n",
    "    plt.ylim((y_test.min()-0.5, y_test.max()+0.5))\n",
    "    plt.xlabel(\"Predictions\")#\n",
    "    plt.ylabel(\"Ground Truth\")\n",
    "    plt.savefig(f\"../../results/synthetic/centroidGP_fold{fold}.png\")\n",
    "    plt.savefig(f\"../../results/synthetic/centroidGP_fold{fold}.pdf\")\n",
    "    plt.savefig(f\"../../results/synthetic/centroidGP_fold{fold}.svg\")\n",
    "    plt.close()\n",
    "\n",
    "print(f\"MAPE\", MAPE)\n",
    "print(f\"RMSE\", RMSE)\n",
    "print(f\"LL\", LL)\n",
    "print(f\"Training Time\", training_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hbz15/MVBAgg/.env/lib/python3.8/site-packages/numpy/core/_methods.py:262: RuntimeWarning: Degrees of freedom <= 0 for slice\n",
      "  ret = _var(a, axis=axis, dtype=dtype, out=out, ddof=ddof,\n",
      "/home/hbz15/MVBAgg/.env/lib/python3.8/site-packages/numpy/core/_methods.py:222: RuntimeWarning: invalid value encountered in true_divide\n",
      "  arrmean = um.true_divide(arrmean, div, out=arrmean, casting='unsafe',\n",
      "/home/hbz15/MVBAgg/.env/lib/python3.8/site-packages/numpy/core/_methods.py:254: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  ret = ret.dtype.type(ret / rcount)\n"
     ]
    }
   ],
   "source": [
    "json_file = json.dumps({\"CV-RMSE\": sum(RMSE) / 5, \"CV-MAPE\": sum(MAPE) / 5, \n",
    "                       \"CV-sd-RMSE\": sem(RMSE), \"CV-sd-MAPE\": sem(MAPE), \"CV-LL\": sum(LL) / 5, \"CV-sd-LL\": sem(LL),\n",
    "                        \"Training Time\": sum(training_time)/5, \"Training Time se\": sem(training_time)}\n",
    "                       )\n",
    "f = open(f\"../../results/synthetic/centroidGP.json\", \"w\")\n",
    "f.write(json_file)\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "&lt;gpflow.models.gpr.GPR object at 0x7f117f6cb760&gt;\n",
       "<table>\n",
       "<thead>\n",
       "<tr><th>name                              </th><th>class    </th><th>transform       </th><th>prior  </th><th>trainable  </th><th>shape  </th><th>dtype  </th><th style=\"text-align: right;\">     value</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>GPR.kernel.kernels[0].variance    </td><td>Parameter</td><td>Softplus        </td><td>       </td><td>True       </td><td>()     </td><td>float64</td><td style=\"text-align: right;\"> 0.144458 </td></tr>\n",
       "<tr><td>GPR.kernel.kernels[0].lengthscales</td><td>Parameter</td><td>Softplus        </td><td>       </td><td>True       </td><td>()     </td><td>float64</td><td style=\"text-align: right;\"> 2.95196  </td></tr>\n",
       "<tr><td>GPR.kernel.kernels[1].variance    </td><td>Parameter</td><td>Softplus        </td><td>       </td><td>True       </td><td>()     </td><td>float64</td><td style=\"text-align: right;\"> 5.89228  </td></tr>\n",
       "<tr><td>GPR.kernel.kernels[1].lengthscales</td><td>Parameter</td><td>Softplus        </td><td>       </td><td>True       </td><td>()     </td><td>float64</td><td style=\"text-align: right;\"> 3.97158  </td></tr>\n",
       "<tr><td>GPR.kernel.kernels[2].variance    </td><td>Parameter</td><td>Softplus        </td><td>       </td><td>True       </td><td>()     </td><td>float64</td><td style=\"text-align: right;\"> 4.68101  </td></tr>\n",
       "<tr><td>GPR.kernel.kernels[2].lengthscales</td><td>Parameter</td><td>Softplus        </td><td>       </td><td>True       </td><td>()     </td><td>float64</td><td style=\"text-align: right;\"> 1.54055  </td></tr>\n",
       "<tr><td>GPR.kernel.kernels[3].variance    </td><td>Parameter</td><td>Softplus        </td><td>       </td><td>True       </td><td>()     </td><td>float64</td><td style=\"text-align: right;\">46.4975   </td></tr>\n",
       "<tr><td>GPR.kernel.kernels[3].lengthscales</td><td>Parameter</td><td>Softplus        </td><td>       </td><td>True       </td><td>()     </td><td>float64</td><td style=\"text-align: right;\"> 5.16387  </td></tr>\n",
       "<tr><td>GPR.likelihood.variance           </td><td>Parameter</td><td>Softplus + Shift</td><td>       </td><td>True       </td><td>()     </td><td>float64</td><td style=\"text-align: right;\"> 0.0123125</td></tr>\n",
       "</tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<gpflow.models.gpr.GPR object at 0x7f117f6cb760>\n",
       "╒════════════════════════════════════╤═══════════╤══════════════════╤═════════╤═════════════╤═════════╤═════════╤════════════╕\n",
       "│ name                               │ class     │ transform        │ prior   │ trainable   │ shape   │ dtype   │      value │\n",
       "╞════════════════════════════════════╪═══════════╪══════════════════╪═════════╪═════════════╪═════════╪═════════╪════════════╡\n",
       "│ GPR.kernel.kernels[0].variance     │ Parameter │ Softplus         │         │ True        │ ()      │ float64 │  0.144458  │\n",
       "├────────────────────────────────────┼───────────┼──────────────────┼─────────┼─────────────┼─────────┼─────────┼────────────┤\n",
       "│ GPR.kernel.kernels[0].lengthscales │ Parameter │ Softplus         │         │ True        │ ()      │ float64 │  2.95196   │\n",
       "├────────────────────────────────────┼───────────┼──────────────────┼─────────┼─────────────┼─────────┼─────────┼────────────┤\n",
       "│ GPR.kernel.kernels[1].variance     │ Parameter │ Softplus         │         │ True        │ ()      │ float64 │  5.89228   │\n",
       "├────────────────────────────────────┼───────────┼──────────────────┼─────────┼─────────────┼─────────┼─────────┼────────────┤\n",
       "│ GPR.kernel.kernels[1].lengthscales │ Parameter │ Softplus         │         │ True        │ ()      │ float64 │  3.97158   │\n",
       "├────────────────────────────────────┼───────────┼──────────────────┼─────────┼─────────────┼─────────┼─────────┼────────────┤\n",
       "│ GPR.kernel.kernels[2].variance     │ Parameter │ Softplus         │         │ True        │ ()      │ float64 │  4.68101   │\n",
       "├────────────────────────────────────┼───────────┼──────────────────┼─────────┼─────────────┼─────────┼─────────┼────────────┤\n",
       "│ GPR.kernel.kernels[2].lengthscales │ Parameter │ Softplus         │         │ True        │ ()      │ float64 │  1.54055   │\n",
       "├────────────────────────────────────┼───────────┼──────────────────┼─────────┼─────────────┼─────────┼─────────┼────────────┤\n",
       "│ GPR.kernel.kernels[3].variance     │ Parameter │ Softplus         │         │ True        │ ()      │ float64 │ 46.4975    │\n",
       "├────────────────────────────────────┼───────────┼──────────────────┼─────────┼─────────────┼─────────┼─────────┼────────────┤\n",
       "│ GPR.kernel.kernels[3].lengthscales │ Parameter │ Softplus         │         │ True        │ ()      │ float64 │  5.16387   │\n",
       "├────────────────────────────────────┼───────────┼──────────────────┼─────────┼─────────────┼─────────┼─────────┼────────────┤\n",
       "│ GPR.likelihood.variance            │ Parameter │ Softplus + Shift │         │ True        │ ()      │ float64 │  0.0123125 │\n",
       "╘════════════════════════════════════╧═══════════╧══════════════════╧═════════╧═════════════╧═════════╧═════════╧════════════╛"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'{\"CV-RMSE\": 0.9457934083113086, \"CV-MAPE\": 0.2689578067378214, \"CV-sd-RMSE\": 0.06792269612314798, \"CV-sd-MAPE\": 0.021991067314917023, \"CV-LL\": 0.5778875262076213, \"CV-sd-LL\": 0.0962754483249235, \"Training Time\": 0.0, \"Training Time se\": NaN}'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "json_file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5it [00:00,  5.30it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MAPE [1.134756454359541, 1.1280068043450413, 1.3186267795295163, 0.9141397598217534, 1.2957029216412572]\n",
      "RMSE [2.3332936594822917, 2.997133030651329, 5.587044419422687, 2.5717109806993723, 2.6883855465343194]\n",
      "Training Time [0.08103799819946289, 0.07909297943115234, 0.07673358917236328, 0.07631373405456543, 0.07688188552856445]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "RMSE = []\n",
    "MAPE  = []\n",
    "LL  = []\n",
    "training_time = []\n",
    "\n",
    "for fold, (train_index, test_index) in tqdm(enumerate(kf.split(keys))):\n",
    "    # randomly pick counties\n",
    "    train_keys = [keys[key] for key in train_index]\n",
    "    test_keys = [keys[key] for key in test_index]\n",
    "    train_index = df_all[\"key\"].isin(train_keys)\n",
    "    test_index = df_all[\"key\"].isin(test_keys)\n",
    "    X_train, X_test = X[train_index], X[test_index]\n",
    "    y_train, y_test = y[train_index], y[test_index]\n",
    "    scaler_x = StandardScaler().fit(X_train)\n",
    "    scaler_y = StandardScaler().fit(y_train)\n",
    "    X_train, y_train = scaler_x.transform(X_train), scaler_y.transform(y_train)\n",
    "    X_test = scaler_x.transform(X_test)\n",
    "\n",
    "    # fit and train GP regression model\n",
    "    m = RandomForestRegressor(max_depth=2, random_state=0)   \n",
    "\n",
    "    t0 = time.time()\n",
    "    m.fit(X_train, y_train.ravel())    \n",
    "    t1 = time.time()\n",
    "\n",
    "    pred = m.predict(X_test)\n",
    "    pred = scaler_y.inverse_transform(pred[:, None])\n",
    "    \n",
    "    # compute metrics \n",
    "    RMSE.append(np.sqrt(np.mean((pred - y_test)**2)))\n",
    "    MAPE.append(np.mean(np.abs(( pred - y_test) / y_test)))\n",
    "    training_time.append(t1-t0)\n",
    "\n",
    "    # plot predictions\n",
    "    plt.figure(figsize=(8,8))\n",
    "    plt.scatter(pred, y_test, color=\"red\")\n",
    "    plt.plot(np.linspace(-100,100,201), np.linspace(-100,100,201), color=\"black\")\n",
    "    plt.xlim((y_test.min()-0.5, y_test.max()+0.5))\n",
    "    plt.ylim((y_test.min()-0.5, y_test.max()+0.5))\n",
    "    plt.xlabel(\"Predictions\")\n",
    "    plt.ylabel(\"Ground Truth\")\n",
    "    plt.savefig(f\"../../results/synthetic/centroidRF_fold{fold}.png\")\n",
    "    plt.savefig(f\"../../results/synthetic/centroidRF_fold{fold}.pdf\")\n",
    "    plt.close()\n",
    "\n",
    "print(f\"MAPE\", MAPE)\n",
    "print(f\"RMSE\", RMSE)\n",
    "print(f\"Training Time\", training_time)\n",
    "\n",
    "json_file = json.dumps({\"CV-RMSE\": sum(RMSE) / 5, \"CV-MAPE\": sum(MAPE) / 5, \n",
    "                       \"CV-sd-RMSE\": sem(RMSE), \"CV-sd-MAPE\": sem(MAPE), \n",
    "                        \"Training Time\": sum(training_time)/5, \"Training Time se\": sem(training_time)}\n",
    "\n",
    "                       )\n",
    "f = open(f\"../../results/synthetic/centroidRF.json\", \"w\")\n",
    "f.write(json_file)\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'{\"CV-RMSE\": 3.2355135273579996, \"CV-MAPE\": 1.1582465439394218, \"CV-sd-RMSE\": 0.5975159552633302, \"CV-sd-MAPE\": 0.07268783390304066, \"Training Time\": 0.07801203727722168, \"Training Time se\": 0.0008977967980275035}'"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "json_file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "b9346729b1e4593c8ff618e5693626c77aa75f12cc79d76918c578b870c1b385"
  },
  "kernelspec": {
   "display_name": "Python 3.8.5 64-bit ('.env': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
